{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1dc3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\Langchain_learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54510450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec2f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    model = \"openai/gpt-oss-120b\",\n",
    "    api_key = os.getenv(\"GROK_API_KEY\"),\n",
    "    temperature = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caef3c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq Q&A Bot Type 'quit' to exit\n",
      "----------------------------------------\n",
      "\n",
      " GROK: Hello! How can I help you today?\n",
      "\n",
      " GROK: **LangChain** is an open‑source framework that makes it easier to build applications powered by large language models (LLMs) such as OpenAI’s GPT‑4, Anthropic’s Claude, Llama 2, and many others.  \n",
      "\n",
      "---\n",
      "\n",
      "## Why it exists\n",
      "LLM‑driven apps often need more than a single “prompt‑and‑response” call. Typical requirements include:\n",
      "\n",
      "| Need | What you usually have to do yourself | How LangChain helps |\n",
      "|------|----------------------------------------|----------------------|\n",
      "| **Prompt composition** | Write and concatenate strings, manage few‑shot examples, keep track of variables | `PromptTemplate`, `FewShotPromptTemplate`, `ChatPromptTemplate` |\n",
      "| **Chaining calls** | Manually call an LLM, feed its output to another LLM or to a tool, handle errors | `Chain`, `SequentialChain`, `SimpleSequentialChain` |\n",
      "| **Memory / state** | Store conversation history, retrieve relevant past info | `Memory` classes (e.g., `ConversationBufferMemory`, `VectorStoreRetrieverMemory`) |\n",
      "| **Tool use / agents** | Write custom code that decides when to call a search API, a calculator, a database, etc. | `Agent` abstractions (`ZeroShotAgent`, `ReActAgent`, `ToolCallingAgent`) |\n",
      "| **Retrieval‑augmented generation (RAG)** | Index documents, embed them, perform similarity search, stitch results into prompts | `VectorStore`, `Retriever`, `RagChain` |\n",
      "| **Evaluation & testing** | Write unit‑style tests for prompts, compare model outputs | `LLMChainEvaluator`, `PromptEvaluator` |\n",
      "| **Deployment** | Wrap everything into a REST endpoint, a FastAPI app, a LangServe server, or a LangChain‑compatible UI | `LangServe`, `LangChainHub` components |\n",
      "\n",
      "All of these pieces are provided as reusable, composable Python (and increasingly JavaScript/TypeScript) classes, so you can focus on the *logic* of your application rather than boilerplate integration code.\n",
      "\n",
      "---\n",
      "\n",
      "## Core Concepts (Python‑centric)\n",
      "\n",
      "| Concept | Description | Typical Class |\n",
      "|---------|-------------|---------------|\n",
      "| **LLM** | The underlying language model (OpenAI, Anthropic, Cohere, local Llama, etc.) | `OpenAI`, `ChatOpenAI`, `Anthropic`, `LLamaCpp` |\n",
      "| **PromptTemplate** | A parametrized prompt string that can be rendered with variables | `PromptTemplate`, `ChatPromptTemplate` |\n",
      "| **LLMChain** | A single step: render a prompt → call an LLM → return the response | `LLMChain(prompt=..., llm=...)` |\n",
      "| **Chain** | A pipeline of two or more `LLMChain`s or other components | `SequentialChain`, `SimpleSequentialChain` |\n",
      "| **Memory** | Stores context across turns (e.g., conversation history) | `ConversationBufferMemory`, `ConversationSummaryMemory` |\n",
      "| **Retriever / VectorStore** | Indexes documents with embeddings and can fetch the most relevant chunks | `FAISS`, `Pinecone`, `Weaviate`, `Chroma` |\n",
      "| **Agent** | Decides which tool to call (search, calculator, DB, etc.) based on LLM output | `ZeroShotAgent`, `ReActAgent`, `ToolCallingAgent` |\n",
      "| **Tool** | Any callable that an agent can invoke (e.g., a web‑search function) | Functions wrapped with `Tool` decorator |\n",
      "| **Callback** | Hook system for logging, streaming, tracing, or visualizing each step | `CallbackManager`, `StdOutCallbackHandler` |\n",
      "| **LangServe** | A lightweight server that automatically turns a chain/agent into a REST API | `run_server` from `langserve` |\n",
      "\n",
      "---\n",
      "\n",
      "## Minimal “Hello‑World” Example\n",
      "\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "# 1️⃣ Define a prompt template\n",
      "prompt = PromptTemplate(\n",
      "    input_variables=[\"topic\"],\n",
      "    template=\"Write a short, witty paragraph about {topic}.\"\n",
      ")\n",
      "\n",
      "# 2️⃣ Choose an LLM (requires OPENAI_API_KEY in env)\n",
      "llm = OpenAI(model_name=\"gpt-4o-mini\", temperature=0.7)\n",
      "\n",
      "# 3️⃣ Wire them together into a chain\n",
      "chain = LLMChain(prompt=prompt, llm=llm)\n",
      "\n",
      "# 4️⃣ Run it\n",
      "print(chain.run({\"topic\": \"quantum computing\"}))\n",
      "```\n",
      "\n",
      "Output (will vary):\n",
      "\n",
      "```\n",
      "Quantum computing is like trying to solve a Rubik's Cube while the colors keep changing—\n",
      "the pieces are in superposition, and every move you make is both right and wrong at the same time.\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## A More Real‑World Pattern: Retrieval‑Augmented Generation (RAG)\n",
      "\n",
      "```python\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "from langchain.chains import RetrievalQA\n",
      "from langchain.llms import OpenAI\n",
      "\n",
      "# 1️⃣ Load & split documents\n",
      "docs = [\"... long text ...\"]   # list of strings or LangChain Document objects\n",
      "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
      "chunks = splitter.split_documents(docs)\n",
      "\n",
      "# 2️⃣ Embed & index\n",
      "embeddings = OpenAIEmbeddings()\n",
      "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "# 3️⃣ Create a retriever\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
      "\n",
      "# 4️⃣ Build a QA chain that pulls relevant chunks into the prompt\n",
      "qa = RetrievalQA.from_chain_type(\n",
      "    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n",
      "    retriever=retriever,\n",
      "    chain_type=\"stuff\",   # \"map_reduce\", \"refine\", etc.\n",
      ")\n",
      "\n",
      "# 5️⃣ Ask a question\n",
      "answer = qa.run(\"What are the main challenges of scaling quantum computers?\")\n",
      "print(answer)\n",
      "```\n",
      "\n",
      "The chain automatically:\n",
      "\n",
      "1. Retrieves the most relevant document chunks.\n",
      "2. Inserts them into a prompt like “Answer the question using only the following context…”.\n",
      "3. Calls the LLM and returns the answer.\n",
      "\n",
      "---\n",
      "\n",
      "## Ecosystem & Extensibility\n",
      "\n",
      "| Area | What LangChain provides | Typical third‑party integrations |\n",
      "|------|------------------------|---------------------------------|\n",
      "| **Embeddings** | `OpenAIEmbeddings`, `HuggingFaceEmbeddings`, `CohereEmbeddings` | `SentenceTransformers`, `Google VertexAI`, `Mistral` |\n",
      "| **Vector Stores** | FAISS (local), Chroma, Pinecone, Weaviate, Milvus, Qdrant, etc. | Any vector DB with a Python client |\n",
      "| **Agents & Tools** | Built‑in search (SerpAPI), calculator, Wikipedia, SQL DB, code interpreter | Custom functions, internal APIs, external SaaS |\n",
      "| **UI / Front‑ends** | `LangChainHub` for sharing prompts/chains, `LangServe` for APIs, `Streamlit`/`Gradio` examples | Any web framework (FastAPI, Flask, Next.js) |\n",
      "| **Observability** | Callback handlers for LangSmith (OpenAI‑partner), LangChain’s own tracing UI, OpenTelemetry adapters | Datadog, Sentry, custom logging |\n",
      "| **Testing** | `PromptEvaluator`, `LLMChainEvaluator`, `MockLLM` | PyTest fixtures, CI pipelines |\n",
      "\n",
      "---\n",
      "\n",
      "## When to Use (and when not to)\n",
      "\n",
      "| Situation | Use LangChain? | Reason |\n",
      "|-----------|----------------|--------|\n",
      "| **Simple one‑shot completion** (e.g., “Summarize this text”) | *Usually not needed* | A direct API call is enough; LangChain adds overhead. |\n",
      "| **Chatbot with memory** | ✅ | `ConversationBufferMemory` or vector‑store‑based memory handles context. |\n",
      "| **Document Q&A or knowledge‑base search** | ✅ | RAG pipelines are built‑in. |\n",
      "| **Complex workflows that call multiple tools** | ✅ | Agents let the LLM decide which tool to invoke. |\n",
      "| **Production‑grade API with monitoring** | ✅ | LangServe + LangSmith give you observability out of the box. |\n",
      "| **Very low‑latency, on‑device inference** | *Maybe* | LangChain works with local models, but the abstraction layers can add latency; a handcrafted pipeline may be leaner. |\n",
      "\n",
      "---\n",
      "\n",
      "## Getting Started\n",
      "\n",
      "```bash\n",
      "# Install core library (Python 3.9+)\n",
      "pip install \"langchain[all]\"   # pulls in most optional deps (FAISS, OpenAI, etc.)\n",
      "\n",
      "# For a lightweight install (only core)\n",
      "pip install langchain\n",
      "```\n",
      "\n",
      "Typical next steps:\n",
      "\n",
      "1. **Set up your API keys** (`OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, etc.) as environment variables.\n",
      "2. **Explore the LangChain Hub** – a public collection of ready‑made prompts, chains, and agents: https://github.com/langchain-ai/langchain-hub\n",
      "3. **Run a quick notebook** – the official repo includes `examples/` notebooks for chatbots, RAG, agents, and more.\n",
      "4. **Deploy with LangServe** if you want an instant REST endpoint:\n",
      "\n",
      "   ```bash\n",
      "   pip install langserve\n",
      "   # my_chain.py contains a `chain = ...` object\n",
      "   langserve run my_chain:chain --host 0.0.0.0 --port 8000\n",
      "   ```\n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR Summary\n",
      "\n",
      "- **LangChain** = a modular, composable toolkit for building LLM‑centric applications.\n",
      "- It provides **prompt templates, chaining logic, memory, retrieval, agents, and observability** out of the box.\n",
      "- You can stitch together **LLM calls, external tools, and vector‑store retrieval** with just a few lines of code.\n",
      "- Ideal for **chatbots, RAG‑based Q&A, autonomous agents, and any workflow where an LLM needs context, tools, or multi‑step reasoning**.\n",
      "- Open‑source, Python‑first (with growing JS/TS support), and integrates with most major LLM providers and vector databases.\n",
      "\n",
      "If you’re looking to move beyond “single‑prompt” usage of GPT‑4 and want a production‑ready way to orchestrate prompts, memory, and tools, LangChain is the de‑facto framework the community has converged on. Happy building!\n",
      "\n",
      " GROK: Hello! How can I assist you today?\n",
      "BYE\n"
     ]
    }
   ],
   "source": [
    "print(\"Groq Q&A Bot Type 'quit' to exit\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nYou\")\n",
    "\n",
    "    if question.lower() == 'quit':\n",
    "        print(\"BYE\")\n",
    "        break\n",
    "    try:\n",
    "        response = llm.invoke(question)\n",
    "        print(f\"\\n GROK: {response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
